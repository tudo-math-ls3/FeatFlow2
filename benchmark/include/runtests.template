#!/bin/sh

# ============================================================================
# Script that runs an arbitrary number of FEAT2 benchmark tests. These tests
# are identified by their benchmark test id (e.g. CC2D_002, POISSON_001,
# FLAGSHIP_TRANSPORT_003_1D, generally any sequence of non-blanks specified as
# value of the keyword 'testid') and defined in *.fbdef files in the
# subdirectory 'tests/'.
#
# The script works on standalone workstations, pc clusters and supercomputers
# like NEC SX-6/-8/-9 and IBM p690 systems.
# ============================================================================


# 'fb_' in function names stands for 'FEAT2 benchmark'.
# Variables are written in capital letters if used globally


# ===================================================================
# Customisation section
# ===================================================================

## Type:	string
## Default:     empty (set e.g. by nightly regression test script)
#
# Major and minor revision of a compiler. To be able to provide and compare
# reference solution for more than one particular compiler revision. The
# string is appended to the fourth token of the build ID.
#
COMPILERVERSION=<SET_VIA_MAKEFILE>


## Type:	boolean integer
## Default:     <SET_VIA_MAKEFILE>, therein set to 0
#
# Whether or not to execute benchmark within a debugger (the particular
# debugger is hard-coded in fb_runAndEvalBenchmarkTest)
#
# Possible values are:
#  * 0
#        normal execution
#  * DDT
#        execute within Allinea's debugger DDT
#  * DBX, DDD, GDB, IDB, PATHDB, PGI, RUN_DBX, TOTALVIEW, VALGRIND
#        execute using the given debugger
#  * <SET_VIA_MAKEFILE>
#        the value is set when invoking 'make *test'. The default value (0)
#        may be overwritten by adding "DEBUGGER=<value>" on the command line,
#        e.g.:         make singletests DEBUGGER=DDT
DEBUGGER=<SET_VIA_MAKEFILE>


## Type:        boolean integer
## Default:     <SET_VIA_MAKEFILE>, therein set to 0
#
# Whether or not to execute subsequently configured refinement levels
# if a test fails on a given refinement level.
#
# Possible values are:
#  * 0
#        skip subsequently configured refinement levels on error
#  * 1
#        try to run all configured refinement levels and any post-run
#        operation
#  * <SET_VIA_MAKEFILE>
#        the value is set when invoking 'make *test'. The default value (0)
#        may be overwritten by adding "KEEPGOING=<value>" on the command line,
#        e.g.:         make singletests KEEPGOING=1
KEEPGOING=<SET_VIA_MAKEFILE>


## Type:	string
## Default:     empty (the appropriate value will be determined automatically)
#
# MPI environment to be used
#
# Depending on this setting the commands
#  * to start an MPI daemon, if any,
#  * to start an MPI programm,
#  * to clean up an MPI environment and
#  * to stop an MPI daemon, if any
# are choosen later on.
#
# Possible values are (for linux32 and linux64 build IDs, other IDs use
# hard-coded values throughout the benchmark)
#
#  * OpenMPI
#  * LAMMPI
#  * MPICH
#  * MPICH2
#  * MVAPICH
#  * <SET_VIA_MAKEFILE>
#        the value is set when invoking 'make *test.fbconf'. The default value
#        (as hardcoded below, arch-dependent) may be overwritten by adding
#        "MPIENV <value>" on the command line,
#        e.g.:         make singletests MPIENV=MPICH2
#
# Whenever changing the syntax here, do not forget to adapt
# bin/lido_schedule_tests as there 'sed' substitutions need to match the line
# derived from this line after 'make <testsuite> <command line arguments>' has
# updated it. Currently, with MPIENV not explicitly overwritten on the make
# command line, it becomes 'MPIENV='.
#
MPIENV=<SET_VIA_MAKEFILE>


## Type:	string
#
# Name of file a benchmark application is supposed to create when it is run
# with relevant data in it that can be compared to previous runs in order to
# validate a program run.
RESULTFILE=benchmarkresultfile
export RESULTFILE


## Type:	string
#
# Name of file to redirect standard output and standard error of a benchmark
# run to. In case of error, the file will be renamed by appending benchmark ID
# and multigrid level.
RUNLOGFILE=runlog


## Type:	boolean integer
## Default:     <SET_VIA_MAKEFILE>, therein set to 0
#
# Quiet or verbose benchmark execution mode.
#
# Possible values are:
#  * 1|yes  (default value)
#        redirect standard output and error to a file (see
#        declaration of RUNLOGFILE). Display its content in case of error.
#  * <SET_VIA_MAKEFILE>
#        the value is set when invoking 'make *test'. The default value (1)
#        may be overwritten by adding "SUPPRESSOUTPUT=<value>" on the command
#        line,
#        e.g.:         make singletests SUPPRESSOUTPUT=0
#  * anything else
#        don't redirect standard output and error. No RUNLOGFILE will be
#        created. Useful for debugging.
SUPPRESSOUTPUT=<SET_VIA_MAKEFILE>


## Type:	string
## Default:     empty
#
# Command line arguments when running valgrind.
VALGRIND_ARGS=""
#VALGRIND_ARGS="${VALGRIND_ARGS} -v"
#VALGRIND_ARGS="${VALGRIND_ARGS} --leak-check=full"
#VALGRIND_ARGS="${VALGRIND_ARGS} --gen-suppressions=all"
#VALGRIND_ARGS="${VALGRIND_ARGS} --suppressions=include/valgrind.suppressions.pc-athlon64-linux32-gcc-blas-ompi.supp"
#VALGRIND_ARGS="${VALGRIND_ARGS} --suppressions=include/valgrind.suppressions.pc-athlon64-linux32-intel-blas-ompi.supp"
#VALGRIND_ARGS="${VALGRIND_ARGS} --suppressions=include/valgrind.suppressions.pc-opteronx2-linux64-gcc-blas-ompi.supp"


# ===================================================================
# End of customisation section
# ===================================================================



fb_initialiseEnv() {
    # (the former header.inc file)

    # Some systems have rather restrictive default umask settings,
    # but typically the results of a nightly benchmark run (used as
    # a regression test) should be accessible by all members of a
    # group, not just by the one who ran the benchmark.
    # (PBS, for instance, enforces a restrictive 0077 umask.)
    umask 0027


    # Empty declaration of variable that will later on contain all environment
    # variables FEAT2 uses in its configuration files. We will
    # always append to this variable, that's why an initialisation is needed.
    addEnvVars=""


    # ===================================================================
    # Section for variable initialisation
    # ===================================================================

    # Initialise variables that keep book of benchmark progress and behaviour.
    testall=0
    testpass=0
    testdeviant=0
    testunverified=0
    testexecfailed=0
    listOfDeviantTests=""
    listOfUnveriTests=""
    listOfCrashedTests=""


    # Environment variables FEAT2 uses in its configuration files that
    # are *not* defined in any *.fbdef file (stored in subdirectory 'tests').
    # Typically, variables defined in this function and in fb_runAndEvalBenchmarkTest
    # should be listed in here.
    # The environment variable "G95_MEM_SEGMENTS=0" tells g95 to suppress the
    # annoying memory statistics at the end of the program run. (The variable does
    # not really fit into the scheme here, but it is the simplest way to pass it
    # to the benchmark.)
    vars2exportAlways="EXECMODE MGLEVEL VISOUTPUTFILE G95_MEM_SEGMENTS"

    # On NEC you might want additional information on a file run
    # on a per file level. Then F_FILEINF needs to be set and
    # exported to all MPI processes.
    #vars2exportAlways="${vars2exportAlways} F_PROGINF F_FILEINF"

    # G95 compiler (www.g95.org) has a feature to show still-allocated memory
    # segments at program end (and where it has been allocated before).
    # Suppress these messages in benchmark.
    G95_MEM_SEGMENTS=0
    export G95_MEM_SEGMENTS
}



fb_footer() {
    header="$1"  # IN
    prefix="$2"  # IN
    footer="$3"  # IN

    if test -z "${header}"; then
       echo
       echo "SUMMARY:"
       echo
    else
       echo ${header}
    fi

    echo ${prefix}"tests in total           : "${testall}
    echo ${prefix}"tests successfully passed: "${testpass}
    echo ${prefix}"tests deviant            : "${testdeviant}
    echo ${prefix}"tests unverifiable       : "${testunverified}
    echo ${prefix}"tests failing execution  : "${testexecfailed}

    # In order that an outer runregressiontest script easily gets to know
    # which tests failed, store the information to file, too.
    #set fileWithAllFailedTests = "failedtests_`hostname`"
    #\rm -f ${fileWithAllFailedTests} || true
    #touch ${fileWithAllFailedTests}

    # Print list of failed tests
    if test ${testdeviant} -gt 0; then
	echo
	echo "The tests with deviating results have the following IDs:"
	echo ${listOfDeviantTests} | tr ' ' '\n'
    fi

    # Print list of tests that could not be verified due to
    # missing reference solution
    if test ${testunverified} -gt 0; then
	echo
	echo "The tests that could not be verified due to a missing"
	echo "reference solution have the following IDs:"
	echo ${listOfUnveriTests} | tr ' ' '\n'
    fi

    # Print list of tests that crashed
    if test ${testexecfailed} -gt 0; then
	echo
	echo "The tests that crashed during execution have the following IDs:"
	echo ${listOfCrashedTests} | tr ' ' '\n'
    fi

    if test -z "${footer}"; then
	echo
    else
	echo ${footer}
    fi
}



fb_raiseUnknownError() {
    echo " "
    echo "No valid FEAT2 log file slaveresult.[0-9] found."
    echo " "
    if test -s ${EXECUTIONLOG}; then
	\cp -p ${EXECUTIONLOG} ${EXECUTIONLOG}.${TESTID}.lev${MGLEVEL} || \
	    echo "Error creating backup of ${EXECUTIONLOG}."
    fi
}



fb_runAndEvalBenchmarkTest() {
    # (the former loop.inc file)

    # set logfile for output during execution and remove contents of previous result comparison
    test -d ${LOGDIR} || mkdir -p ${LOGDIR}
    EXECUTIONLOG=${LOGDIR}/${RUNLOGFILE}
    \rm ${LOGDIR}/${RESULTFILE} 1>/dev/null 2>&1

    # print out some general information
    echo "TESTID    = "${TESTID}
    echo "APP       = "${appname}

    # add some class-specific information, add whatever you want to see for your applications
    case "${CLASS}" in

	# Standard applications. They have a DAT file and some MG-Levels.

	DEFAULT)
	    echo "DATFILE   = "${DATFILE}
	    echo "MGLEVELS  = "${MGLEVELS}
	    if test "${FOOBAR+set}" = set ; then
		echo "FOOBAR    = "${FOOBAR}
	    elif test "${FOOBAZ+set}" = set ; then
		echo "FOOBAZ    = "${FOOBAZ}
	    fi
	    ;;

	# Matthias' apps:
	FLAGSHIP)
	    echo "APPFLAGS  = "${APPFLAGS}
	    echo "DATFILE   = "${DATFILE}
	    echo "MGLEVELS  = "${MGLEVELS}
	    if test "${FOOBAR+set}" = set ; then
		echo "FOOBAR    = "${FOOBAR}
	    elif test "${FOOBAZ+set}" = set ; then
		echo "FOOBAZ    = "${FOOBAZ}
	    fi

	    # This is a slight extension to the default behaviour.
	    # Since the flagship application supports command line arguments
	    # we prepend the contents of APPFLAGS to the variable DATFILE so
	    # that additional FLAGS and the DAT file are passed to the
	    # executable
	    #
	    # Drawback: It breaks the possibility to run this script with
	    #           anything else than DEBUGGER=0. DEBUGGER=PGI, e.g.,
	    #           will complain because pgdbg will interpret the command
	    #           line options for the program to be debugged as command
	    #           line options for the debugger itself.
	    DATFILE="${APPFLAGS} ${DATFILE}"
	    echo "DATFILE   = "${DATFILE}
	    ;;

	*)
	    echo "# Warning: The function fb_runAndEvalBenchmarkTest is not prepared to handle application class <${CLASS}>."
    esac

    # print out some general information (continued)

    echo

    startdate=`date -u '+%Y %m %d %H %M %S'`  # Format due to awk's mktime() input requirements
    echo "START : "`date`
    echo

    # #########################
    # non-verbose mode
    outputredirect=""
    if test "${SUPPRESSOUTPUT}" = "1" -o "${SUPPRESSOUTPUT}" = "yes"; then
	outputredirect=" >> ${EXECUTIONLOG}"
    fi

    # Execute only of FEAT2 benchmark binary is available
    if test -r ${appname}; then
	# Test binary for compatibility
	ARCH="`uname -m`";
	case ${ARCH} in
	    sun4u)
		string="SPARC"
		;;

	    alpha)
		string="alpha.*executable"
		;;

	    i*86)
		string="Intel 80386"
		;;

	    x86_64)
		string="ELF 64-bit LSB *executable"
		;;

	    00223C3B4C00) # IBM p690
		string="64-bit XCOFF"
		;;

	    *)
		# No compatibility testing for unhandled architecture.
		string=" "
	esac
	stdoutput=`POSIXLY_CORRECT=1 file ${appname} | grep "${string}"`
	if test -z "${stdoutput}"; then
	    # Binary incompatible
	    echo "Binary <${appname}> incompatible. Please re-make / re-link."

	else
	    # Binary compatible, run test

	    # Initialise boolean variable
	    execfailed=0
	    EXECMODE="`echo ${EXECMODE} | tr '[a-z]' '[A-Z]'`"

	    case "${CLASS}" in
		# OpenGL tests need write permission to NVIDIA device.
		# Enable this.
		COPROC*)
		    test -s /var/spool/gpuauth && xauth nmerge /var/spool/gpuauth
		    ;;
	    esac


	    # Loop over all multigrid levels
	    for currentmglevel in `echo ${MGLEVELS} | sed 's/,/ /g'`; do
		# If execution failed in a previous loop, exit the loop. Why?
		# Well, after all: Failed is failed.
		if test "${KEEPGOING}" = "0" -a ${execfailed} -eq 1; then
		    break
		fi
		MGLEVEL=${currentmglevel}
		export MGLEVEL

		\rm ${EXECUTIONLOG} 1>/dev/null 2>&1


		# Passing an argument to the program to be debugged does not work.
		# Store config file under default config file name.
		# This copy operation is limited to debug sessions as debug sessions are
		# usually performed one at a time. In a scheduling environment, several
		# copy/remove operations might occur simultaneously - which might raise
		# problems.
#		if test "${DEBUGGER}" != "0"; then
#		    \cp -p ${DATFILE} master.dat
#		fi

		# ############################
		# MPI
		case "${EXECMODE}" in
		    # ############################
		    MPI)
			echo "Unsupported in FEAT2" 1>&2
			;;


		    # ############################
		    SERIAL)
			# To select a debugger, (un)comment the appropriate
			# lines. While being uncomfortable, this approach is
			# considered as 'has to do' by the fb2 developers for
			# the time being.  If you believe that this sucks,
			# please implement this feature properly by allowing
			# multiple string values for DEBUGGER= and
			# select-case'ing them here.

			# Please remember that debugging optimised builds
			# requires changing compiler flags in
			# kernel/arch/templates to include -g which tells the
			# compiler to include
			# routine names, variable names etc. in the binary

			case "`echo ${DEBUGGER} | tr '[a-z]' '[A-Z]'`" in
			    # serial run, normal execution
			    0)
				use_numactl=0
				# check if numactl is available
				if which numactl 1> /dev/null 2>&1; then
				    # check whether it is usable
				    if numactl --show 1> /dev/null 2>&1 ; then
					use_numactl=1;
				    fi
				fi
				if test $use_numactl -eq 1; then
				    printf "Running "${currentmglevel}" (with NUMACTL).... "
#				    echo numactl --cpubind=0 --membind=0 ${PWD}/${appname} ${DATFILE}
				    eval numactl --cpubind=0 --membind=0 ${PWD}/${appname} ${DATFILE} ${outputredirect}
				else
				    printf "Running "${currentmglevel}" (without NUMACTL).... "
				    eval ${PWD}/${appname} ${DATFILE} ${outputredirect}
				fi
				;;

			    DDT)
				printf "Running "${currentmglevel}".... "

				# Issue warning when Intel, PGI or Pathscale compiler are being used.
				if test -n "`echo ${BUILDID} | grep intel-`" -o \
					-n "`echo ${BUILDID} | grep pgi-`" -o \
					-n "`echo ${BUILDID} | grep psc-`"; then
				    echo
				    echo "Warning! Warning! Warning! Warning! Warning! Warning!"
				    echo
				    echo "When debugging a program, try not to compile it with Intel Compilers and "
				    echo "debug it with gdb (plain or as backend in ddt). You will experience very "
				    echo "serious performance penalties, communication between ddt and gdb will "
				    echo "time-out very often. Support for idb in ddt exists, but according to the"
				    echo "ddt developers, the effort Intel (and Pathscale and PGI) invest into their"
				    echo "debuggers is significantly less than what the GNU community does. Instead"
				    echo "of using idb, try to trigger the bug and then debug the code with a gcc-compiled"
				    echo "binary!"
				    echo
				    echo "Warning! Warning! Warning! Warning! Warning! Warning!"
				    echo
				fi

				ddt ${PWD}/${appname} ${DATFILE}
				;;

			    VALGRIND)
				# use valgrind
				printf "Running "${currentmglevel}".... "
				eval valgrind ${VALGRIND_ARGS} ${PWD}/${appname} ${DATFILE} ${outputredirect}
				;;

			    GDB)
				printf "Running "${currentmglevel}".... "

				# use gdb
				echo
				echo "Warning! Warning! Warning! Warning! Warning! Warning!"
				echo
				echo "Never try to perform debugging without SUPPRESSOUTPUT=0, as the"
				echo "debugger otherwise seems to hang the machine waiting for you to type run."
				echo
				echo "Warning! Warning! Warning! Warning! Warning! Warning!"
				echo
				echo
				echo "Note: To start debugging, issue 'run ${DATFILE}' at the gdb command prompt"
				echo
				echo
				eval gdb ${PWD}/${appname} ${outputredirect}
				;;

			    DBX)
				printf "Running "${currentmglevel}".... "
				eval dbx -r  ${PWD}/${appname} ${DATFILE} ${outputredirect}
				;;

			    RUN_DBX)
				printf "Running "${currentmglevel}".... "
				# run_dbx settings
				eval run_dbx ${PWD}/${appname} ${DATFILE} ${outputredirect}
				;;

			    DDD)
				printf "Running "${currentmglevel}".... "
				ddd ${PWD}/${appname} ${DATFILE}
				;;

			    PATHDB)
				printf "Running "${currentmglevel}".... "
				pathdb ${PWD}/${appname} ${DATFILE}
				;;

			    PGI)
				printf "Running "${currentmglevel}".... "
				pgdbg ${PWD}/${appname} ${DATFILE}
				;;

			esac


			# Show problems if any occured
			if test $? -ne 0; then
			    execfailed=1
			    if test "${SUPPRESSOUTPUT}" = "1" -o "${SUPPRESSOUTPUT}" = "yes"; then
				echo "Execution failed:"
				cat ${EXECUTIONLOG}

				\cp -p ${EXECUTIONLOG} ${EXECUTIONLOG}.${TESTID}.lev${MGLEVEL}
				if test $? -ne 0; then
				    echo "Error creating backup of ${EXECUTIONLOG}."
				fi
			    else
				echo "Execution failed."
			    fi
			else
			    echo "Done"
			fi
			;;
		esac # serial/MPI


		# This remove operation is limited to debug sessions as debug sessions are
		# usually performed one at a time. In a scheduling environment, several
		# copy/remove operations might occur simultaneously - which might raise
		# problems.
#		if test "${DEBUGGER}" != "0"; then
#		    rm -f master.dat
#		fi


		# Aggregate the content of the benchmark result file
		# over all refinement levels.
		if test ${execfailed} -ne 1 -a -s ${LOGDIR}/${RESULTFILE}; then
		    cat ${LOGDIR}/${RESULTFILE} >> ${LOGDIR}/${RESULTFILE}.all-levels
		fi


		# Every application is supposed to output data that can be compared
		# to previous results. Preferably line per program run.
		# To provide utmost flexibility, based on either application name or
		# test class one can add tailored instructions here, e.g. to provide
		# a header for the table like program output.
		case "${CLASS}" in
		    CC2D)
			TABLE_HEADER=''
			;;

		    FLAGSHIP)
			egrep "^\* (Warning|Error)" ${LOGDIR}/output.log
			TABLE_HEADER=''
			;;

		    *)
			TABLE_HEADER="
WARNING! For this application / test class there are no instructions
         coded in runtests.template that would generate output
         (preferably a single line per program run) that can be
         compared against a reference output. Resorting as a fail back
         to this program run's complete screen output - which is
         considered lazy and bad practice, because the complete output
         is prone to frequently change, but then only marginally!
"
			;;
		esac
	    done # end loop over all refinement levels


	    # Determine filename for the results and remove file if its exists
	    FILENAME="test"${TESTID}".result"
	    if test "${DIFFERENTSERIALRESULT}" = "true" -o "${DIFFERENTSERIALRESULT}" = "1"; then
		case "${EXECMODE}" in
		    MPI)
			FILENAME="test"${TESTID}".mpi.result"
			;;
		    SERIAL)
			FILENAME="test"${TESTID}".serial.result"
			;;
		esac
	    fi
	    PATHTORESULTFILE=results/${BUILDID}
	    PATHTOREFERENCEFILE=refsol/${BUILDID}
	    # Squeeze in COMPILERVERSION
	    PATHTORESULTFILE=`echo ${PATHTORESULTFILE} | sed -e "s/^\([^-][^-]*-[^-][^-]*-[^-][^-]*-[^-][^-]*\)\(.*\$\)/\1${COMPILERVERSION}\2/"`
	    PATHTOREFERENCEFILE=`echo ${PATHTOREFERENCEFILE} | sed -e "s/^\([^-][^-]*-[^-][^-]*-[^-][^-]*-[^-][^-]*\)\(.*\$\)/\1${COMPILERVERSION}\2/"`
	    # Create directory structure where results are stored
	    # if necessary
	    if test ! -d ${PATHTORESULTFILE}; then
		mkdir -p ${PATHTORESULTFILE}
	    fi
	    test -s ${PATHTORESULTFILE}/${FILENAME} && rm -f ${PATHTORESULTFILE}/${FILENAME}

	    testall=`expr ${testall} + 1`

	    # Execution error or missing results?
	    if test ${execfailed} -eq 1 -o ! -s ${LOGDIR}/${RESULTFILE}.all-levels; then
		echo "TEST FAILED"
		testexecfailed=`expr ${testexecfailed} + 1`
		listOfCrashedTests=`echo ${listOfCrashedTests} ${TESTID}`

		if test ${execfailed} -eq 1; then
		    echo
		    echo "No information on FEAT2 run available that can be compared against a"
		    echo "reference solution due to execution failure."
		    echo
		else
		    echo
		    echo "No information on FEAT2 run available that can be compared against a"
		    echo "reference solution due to missing or empty log file"
		    echo "<${LOGDIR}/${RESULTFILE}.all-levels>."
		    echo
		fi

		echo "Displaying fragmented results."
		if test -s ${LOGDIR}/${RESULTFILE}.all-levels; then
		    echo "${TABLE_HEADER}"
		    cat ${LOGDIR}/${RESULTFILE}.all-levels
		    echo

		    # Store fragmented results to disk
		    \cp ${LOGDIR}/${RESULTFILE}.all-levels ${PATHTORESULTFILE}/${FILENAME}
		else
		    echo "[None available.]"
		    echo
		fi

	    # Everything's fine, compare result with reference solution
	    else
		echo
		echo "Storing results."
		# Store results to disk
		\cp ${LOGDIR}/${RESULTFILE}.all-levels ${PATHTORESULTFILE}/${FILENAME}

		# Compare with reference solution
		if test -r ${PATHTOREFERENCEFILE}/${FILENAME}; then
		    echo "Comparing reference with current solution..."
		    diff -w ${PATHTOREFERENCEFILE}/${FILENAME} ${PATHTORESULTFILE}/${FILENAME} 1> ${LOGDIR}/diffresult 2>&1
		    # Check whether reference and current solution match
		    if test $? -eq 0; then
			echo "TEST OK"
			testpass=`expr ${testpass} + 1`
			echo
			echo "${TABLE_HEADER}"
			cat ${PATHTORESULTFILE}/${FILENAME}
			echo
		    else
			echo "TEST DEVIANT"
			testdeviant=`expr ${testdeviant} + 1`
			listOfDeviantTests=`echo ${listOfDeviantTests} ${TESTID}`
			echo
			echo "${TABLE_HEADER}"
			cat ${LOGDIR}/diffresult
			echo
			echo "Difference:"
#			perl include/diff_refsol_result.pl "${APPL}" ${PATHTOREFERENCEFILE}/${FILENAME} ${PATHTORESULTFILE}/${FILENAME}
			diff ${PATHTOREFERENCEFILE}/${FILENAME} ${PATHTORESULTFILE}/${FILENAME}
			echo
		    fi
		else
		    # No reference solution available
		    echo "TEST WITHOUT REFERENCE SOLUTION"
		    testunverified=`expr ${testunverified} + 1`
		    listOfUnveriTests=`echo ${listOfUnveriTests} ${TESTID}`
		    echo
		    echo "${TABLE_HEADER}"
		    cat ${PATHTORESULTFILE}/${FILENAME}
		    echo
		fi
	    fi

	    if test ${execfailed} -eq 0 -o "${KEEPGOING}" != "0"; then
		# For some applications a couple of postprocessing steps,
		# after having done all the refinement levels, is wanted. E.g.
		# to compute L2 error reduction rates
		case "${APPL}" in
		    FEASTbasicops)
			# Store settings used to generate the binary that runs
			# the benchmark test IDs SBBLAS* for later evaluation
			# using the script fbenchmark2/bin/generate_sbblasconf.
			if test "${TESTFEATURE}" = "sparse_banded_blas"; then
			    ( cd src_basicops;
			      printf "Hostname           : ";
			      hostname;
			      printf "Date               : ";
			      date;
			      printf "Configuration      : ";
			      make --no-print-directory .libidonly;
			      printf "Compiler version   : ";
			      make --no-print-directory .sbblascompilerversion;
			      printf "Compiler options   : ";
			      make --no-print-directory .sbblascompileroptions; ) > ${LOGDIR}/compilersettings;
			fi
			;;

		    FEASTmatmod)
			;;

		    FEASTnavsto|FEASTstokes)
			echo "error reduction rates"
			if test -s ${PATHTORESULTFILE}/${FILENAME}; then
			    perl include/comparenavstokes.pl ${PATHTORESULTFILE}/${FILENAME}
			else
			    echo "[None available.]"
			    echo
			fi
			;;
		esac
	    fi

	    # Clean up
#	    /bin/rm ${EXECUTIONLOG} diffresult 1> /dev/null 2>&1
	fi # binary compatible/incompatible
    else
	# No binary found
	echo
	echo "FEAT2 benchmark binary <${appname}> not found."
	echo "TEST FAILED"
	echo
	testall=`expr ${testall} + 1`
	testexecfailed=`expr ${testexecfailed} + 1`
	listOfCrashedTests=`echo ${listOfCrashedTests} ${TESTID}`
    fi

    echo

    enddate=`date -u '+%Y %m %d %H %M %S'`  # Format due to awk's mktime() input requirements
    echo "FINISH  : "`date`

    # run time?
    awk 'BEGIN {
         # convert input to epoch
         start = mktime(ARGV[1]);
         end   = mktime(ARGV[2]);
         # difference in seconds
         diff  = end - start;
         # Problem 1:
         # strftime("%j", diff) returns number of days since January, 1st.
         # So, if the run time is more than a day it will return 2 instead of 1.
         # Simply subtracting 1 does not do the trick as then
         #     strftime("%j:%T", diff);
         # would return "01:-1:55:39" for input start="2012 06 05 12 11 56",
         # end="2012 06 06 12 07 35". So, do it manually for now.
         days = 0;
         if (diff > 86400) {
             days = int(diff / 86400);
             diff -= 86400 * days;
         }
         hour = int(diff / 3600);
         diff -= 3600 * hour;
         min = int(diff / 60);
         sec = diff - 60 * min;
         if (sec < 0) { sec += 60; min -= 1; }
         if (min < 0) { min += 60; hour -= 1; }
         if (hour < 0) { hour += 24; days -= 1; }
         if (days > 0) {
             printf "RUNTIME : %02d:%02d:%02d:%02d\n", days, hour, min, sec;
         } else {
             printf "RUNTIME : %02d:%02d:%02d\n", hour, min, sec;
         }
    }' "$startdate" "$enddate"

    # Show test statistics if user presses or has pressed a key - if stdin is a terminal.
    #
    # To accomplish this with shell builtins we would require 'read' to support a
    # timeout option which is not available in plain sh. Bash (at least version 3.2 and
    # higher, not sure about earlier versions) supports it, but bash 4.1.5 tends to
    # choke on named pipe used elsewhere in this script (at least in 1% of cases)
    # leading to spurious test crashes unrelated to programming or input errors. So, the
    # bash shell builtin 'read' is not an option either.
    # Rely on the Perl module Term::ReadKey - which may not be available, though
    if test -t 0; then  # STDIN typically has file descriptor 0
       if perl -c include/query_key_pressed.pl 1>/dev/null 2>/dev/null; then
           if ! perl include/query_key_pressed.pl; then
               echo
               echo
               fb_footer "+---- intermediate summary ----" "|" "+------------------------------";
           fi
       fi
    fi

    echo " "
    echo " "
    echo " "
}




# ===================================================================
# Run the benchmark
# ===================================================================

# Clean up any temporary files from previous benchmark runs
#\rm *.gmv *.gmv.[0-9]* 1> /dev/null 2>&1

fb_initialiseEnv
