#!/bin/sh

# 'fb_' stands for 'FEAT2 benchmark'.
# Variables are written in capital letters if used globally.


# Initialisations
ALLTESTS=
AFTERJOB=""
BENCHMARKRUNSCRIPT=runtests
COMPILERVERSION=
DOSENDMAIL=
EMAILADDRESS=
FORCE_COMPLETE_NODES=1    # If FORCE_COMPLETE_NODES=0, then the term
                          # 'nodes' in variable names actually refers
                          # to cores (e.g. in CORESPERNODE,
                          # CORESPERQUADNODE, eth_maxavailnodes[]).
FORCE_USE_ALL_CORES_PER_NODE=0  # If set to 0, only 50% of all cores in a
                                # compute node will be used.
FORCE_USE_QUAD_QUEUES=0
FORCE_USE_NEHALEM_QUEUE=0
SINGLE_NODE_JOBS_ALLOWED_IN_IB_QUEUE=1  # Set to 1, if PBS setting 'resources_min.ncpus' is unset or set to not more than 1.
GNUMAKE=
JOBFILEPREFIX="feat2job"
MODULESLOADED=
NOSCHEDULE=0
NTASKS=
OVERWRITE_LOG_DIRECTORY=
WALLCLOCKLIMIT="00:10:00"

case "`hostname -f`" in
    *.lidocluster.hp|lido*.itmc.tu-dortmund.de)
        # Torque on LiDO-2 is configured to believe every node has
        # only four cores. Despite it having 8. But using 8 cores
        # would give a serious performance penalty (memory wall) given
        # that there is only one memory bus per node.
        CORESPERNODE=4;            # valid for nodes with property std, ib, nehalem
        MAXCORESUSEDPERNODE=4;
        EXTRACORESPERNODEFACTOR=1;
        CORESPERQUADNODE=8;        # valid for nodes with property quad
        MAXCORESUSEDPERQUADNODE=8;
        if test $FORCE_USE_ALL_CORES_PER_NODE -eq 1; then
            # Use up to EXTRACORESPERNODEFACTOR * MAXCORESUSEDPERNODE cores per compute node. A
            # value > 1 makes sense on LiDO-2 if we want to use all cores in a compute node because
            # then we need to ask Torque for less compute nodes.
            EXTRACORESPERNODEFACTOR=2;
        fi
        ;;
    *)
        cat <<EOF 1>&2
$0: Error: Command 'hostname' returned an unhandled string. Script cancelled.
EOF
        exit 1
        ;;
esac


# ==================================================================
# IMPORTANT
# ==================================================================
# It is assumed this script is called from a FEAT2 directory.
# This assumption is not only made here, but also later as the
# Makefile in a benchmark directory (whatever its actual name is)
# is needed to create a 'runtests' script. The library function
# fb_ensureCorrectWorkingDir even checks whether that's the case.


# ==================================================
# = Load library with functions needed for         =
# = every scheduling script (NEC, LiDO, JUMP etc.) =
# ==================================================
. include/lib_for_xxx_schedule_tests || exit 2


# Three arrays containing relevant LiDO queue settings
# Indices must always be the same:
# 0 = short_*  ,  1 = med_*  ,  2 = long_*
#
# Names of queues, maximum wallclock times (in seconds) and
# maximum virtual memory use as well as
# maximum number of nodes currently available (= online nodes)
# (FIXME: 'declare' is not a builtin command of POSIX 1003.2 shell.
#         remove this variable declaration entirely?)
numQueues=3
declare -a queuename[${numQueues}], wallclocksecondslimit[${numQueues}]
queuename[0]="short"
queuename[1]="med"
queuename[2]="long"
declare -a eth_vmemlimit[${numQueues}], ib_vmemlimit[${numQueues}], nehalem_vmemlimit[${numQueues}], quad_vmemlimit[${numQueues}]
declare -a eth_maxavailnodes[${numQueues}], ib_maxavailnodes[${numQueues}], nehalem_maxavailnodes[${numQueues}], quad_maxavailnodes[${numQueues}]
declare -a eth_maxavailcores[${numQueues}], ib_maxavailcores[${numQueues}], nehalem_maxavailcores[${numQueues}], quad_maxavailcores[${numQueues}]


#  Function: Query PBS to find out about available nodes and maximum walltime per queue
fb_queryPBSLiDO() {
    # Create temporary file (with unique file name)
    queues=`mktemp`
    if test $? -ne 0; then
        cat <<EOF 1>&2
$0: Error: Creating temporary file failed. Script cancelled.
EOF
        exit 3
    fi

    # Query PBS queue manager and store result in temporary file
    qmgr -c 'print queue @master' > ${queues}
    if test $? -ne 0; then
        cat <<EOF 1>&2
$0: Error: Could not query PBS manager for available resources. Script cancelled.
EOF
        exit 4
    fi

    nodes=`mktemp`
    if test $? -ne 0; then
        cat <<EOF 1>&2
$0: Error: Creating temporary file failed. Script cancelled.
EOF
        exit 5
    fi

    # Query PBS queue manager and store result in temporary file
    qmgr -c 'print node @master' > ${nodes}
    if test $? -ne 0; then
        cat <<EOF 1>&2
$0: Error: Could not query PBS manager for available nodes. Script cancelled.
EOF
        exit 6
    fi

    # Define a smaller helper routine for code repeatedly needed
    # Query a given queue for its hosts, its vmem and walltime limits
    fb_getSettingsForQueue() {
        queue="$1"  # IN
        # (Sed:) Unwrap all lines belonging to a single queue definition
        sed -e '/^#/d;' -e :a -e '$!N;s/\n/; /;ta' -e 's/; create queue/\ncreate queue/g' ${queues} | \
                # Remove queues not open to public
                egrep -v "(acl_user_enable = True; .* acl_users = \-;|acl_group_enable = True; .* acl_groups = \-)" | \
                # Extract current queue
                grep -w "create queue ${queue}"
    }

    # Define another smaller helper routine for code repeatedly needed
    # Query server for all compute node properties
    fb_getSettingsForNodes() {
        nodeProperty="$1"  # IN
        # (Sed:) Unwrap all lines belonging to a single queue definition
        sed -e '/^#/d;' -e :a -e '$!N;s/\n/; /;ta' -e 's/; create node/\ncreate node/g' ${nodes} | \
                # Remove offline/down nodes
                egrep -v " state = (down|offline);" | \
                # Extract current node property
                grep " properties = [^;]*\b${nodeProperty}\b"
    }

    # queue settings
    for ((i=0; i < ${numQueues}; i++)); do
        queue=${queuename[$i]};
        # Determine maximum wall clock time
        wallclocksecondslimit[$i]=`fb_getSettingsForQueue $queue | sed -e 's/^.*resources_max.walltime = \([0-9:][0-9:]*\)[^0-9:]*.*$/\1/' | awk -F: '{print $1*3600+$2*60+$3}'`
    done

    # nodes with standard ethernet interconnect
    for ((i=0; i < ${numQueues}; i++)); do
        nodeProperty=std
        # Determine maximum virtual memory use
        eth_vmemlimit[$i]=`fb_getSettingsForNodes ${nodeProperty} | sed -e 's/^.* status += physmem=\([0-9][0-9]*[kmgt]b\);.*$/\1/' | sort -gr | head -n 1`
        # Determine maximum number of nodes available
        eth_maxavailnodes[$i]=`fb_getSettingsForNodes ${nodeProperty} | grep -c '^create node'`
        # Determine maximum number of cores available
        eth_maxavailcores[$i]=`fb_getSettingsForNodes ${nodeProperty} | awk '{ if (/^create node/) { sum += gensub(/^.* np = ([0-9][0-9]*);.*/, "\\\\1", "g"); }} END { print sum; }'`
    done

    # nodes with infiniband interconnect
    for ((i=0; i < ${numQueues}; i++)); do
      nodeProperty=ib
      # Determine maximum virtual memory use
      ib_vmemlimit[$i]=`fb_getSettingsForNodes ${nodeProperty} | sed -e 's/^.* status += physmem=\([0-9][0-9]*[kmgt]b\);.*$/\1/' | sort -gr | head -n 1`
      # Determine maximum number of nodes available
      ib_maxavailnodes[$i]=`fb_getSettingsForNodes ${nodeProperty} | grep -c '^create node'`
      # Determine maximum number of cores available
      ib_maxavailcores[$i]=`fb_getSettingsForNodes ${nodeProperty} | awk '{ if (/^create node/) { sum += gensub(/^.* np = ([0-9][0-9]*);.*/, "\\\\1", "g"); }} END { print sum; }'`
    done

    # quad nodes
    for ((i=0; i < ${numQueues}; i++)); do
      queue=${queuename[$i]};
      nodeProperty=quad
      # Determine maximum virtual memory use
      quad_vmemlimit[$i]=`fb_getSettingsForNodes ${nodeProperty} | sed -e 's/^.* status += physmem=\([0-9][0-9]*[kmgt]b\);.*$/\1/' | sort -gr | head -n 1`
      # Determine maximum number of nodes available
      quad_maxavailnodes[$i]=`fb_getSettingsForNodes ${nodeProperty} | grep -c '^create node'`
      # Determine maximum number of cores available
      quad_maxavailcores[$i]=`fb_getSettingsForNodes ${nodeProperty} | awk '{ if (/^create node/) { sum += gensub(/^.* np = ([0-9][0-9]*);.*/, "\\\\1", "g"); }} END { print sum; }'`
    done

    # nehalem nodes
    for ((i=0; i < ${numQueues}; i++)); do
      queue=${queuename[$i]};
      nodeProperty=nehalem
      # Determine maximum virtual memory use
      nehalem_vmemlimit[$i]=`fb_getSettingsForNodes ${nodeProperty} | sed -e 's/^.* status += physmem=\([0-9][0-9]*[kmgt]b\);.*$/\1/' | sort -gr | head -n 1`
      # Determine maximum number of nodes available
      nehalem_maxavailnodes[$i]=`fb_getSettingsForNodes ${nodeProperty} | grep -c '^create node'`
      # Determine maximum number of cores available
      nehalem_maxavailcores[$i]=`fb_getSettingsForNodes ${nodeProperty} | awk '{ if (/^create node/) { sum += gensub(/^.* np = ([0-9][0-9]*);.*$/, "\\1", 1); }} END { print sum; }'`
    done

    rm -f ${queues} ${nodes}
}


#  Function: Determine most suitable queue depending on node, walltime and interconnect requirements
fb_determineQueue() {
    nodes="$1"         # IN (format: integer)
    nodeProperty="$3"  # IN (valid: "std", "ib", "quad", "nehalem")

    walltimeseconds="`echo $2 | awk -F: '{print $1*3600+$2*60+$3}'`"   # IN (format: hh:mm:ss)


    # Check nehalem queue
    if test "${nodeProperty}" = "nehalem"; then
	for ((i=0; i < ${numQueues}; i++)); do
            queue=${queuename[$i]};
            # Check whether enough nodes in queue available
            if test ${nodes} -le ${nehalem_maxavailnodes[$i]}; then
                # Check whether wall time limits are not exceeded
                if test ${walltimeseconds} -le ${wallclocksecondslimit[$i]}; then
                    echo ${queue};
                    return;
                fi
            fi
        done
    fi


    # Check quadcore queues
    if test "${nodeProperty}" = "quad"; then
	for ((i=0; i < ${numQueues}; i++)); do
            queue=${queuename[$i]};
            # Check whether enough nodes in queue available
            if test ${nodes} -le ${quad_maxavailnodes[$i]}; then
                # Check whether wall time limits are not exceeded
                if test ${walltimeseconds} -le ${wallclocksecondslimit[$i]}; then
                    echo ${queue};
                    return;
                fi
            fi
        done
    fi


    # Check ethernet queues
    # (Force use of ethernet queue for serial jobs - if ib nodes require >= 2 nodes usage and if nehalem queue is not requested)
    if test "${nodeProperty}" = "std" -o \( "${SINGLE_NODE_JOBS_ALLOWED_IN_IB_QUEUE}" = 0 -a "${nodes}" = 1 \); then
	for ((i=0; i < ${numQueues}; i++)); do
            queue=${queuename[$i]};
            # Check whether enough nodes in queue available
            if test ${nodes} -le ${eth_maxavailnodes[$i]}; then
                # Check whether wall time limits are not exceeded
                if test ${walltimeseconds} -le ${wallclocksecondslimit[$i]}; then
                    echo ${queue};
                    return;
                fi
            fi
        done
    fi


    # Check infiniband queues
    # (Prohibit use of infiniband queue for serial jobs - if not permitted)
    if test "${nodeProperty}" = "ib" -a \( "${SINGLE_NODE_JOBS_ALLOWED_IN_IB_QUEUE}" = 1 -o "${SINGLE_NODE_JOBS_ALLOWED_IN_IB_QUEUE}" = 0 -a "${nodes}" != 1 \); then
	for ((i=0; i < ${numQueues}; i++)); do
            queue=${queuename[$i]};
            # Check whether enough nodes in queue available
            if test ${nodes} -le ${ib_maxavailnodes[$i]}; then
                # Check whether wall time limits are not exceeded
                if test ${walltimeseconds} -le ${wallclocksecondslimit[$i]}; then
                    echo ${queue};
                    return;
                fi
            fi
        done
    fi
}


#  Function: Determine vmem limit for a given node property
fb_getVMemLimitForNodeProperty() {
    nodeProperty="$1"  # IN

    case "${nodeProperty}" in
        std)
	    for ((i=0; i < ${numQueues}; i++)); do
                if test "${queue}x" = "${queuename[$i]}x"; then
                    echo ${eth_vmemlimit[$i]};
                    return;
                fi
            done
            ;;
        ib)
	    for ((i=0; i < ${numQueues}; i++)); do
                if test "${queue}x" = "${queuename[$i]}x"; then
                    echo ${ib_vmemlimit[$i]};
                    return;
                fi
            done
            ;;
        nehalem)
	    for ((i=0; i < ${numQueues}; i++)); do
                if test "${queue}x" = "${queuename[$i]}x"; then
                    echo ${nehalem_vmemlimit[$i]};
                    return;
                fi
            done
            ;;
        quad)
	    for ((i=0; i < ${numQueues}; i++)); do
                if test "${queue}x" = "${queuename[$i]}x"; then
                    echo ${quad_vmemlimit[$i]};
                    return;
                fi
            done
            ;;
    esac

    echo "0kb";
    return
}


#  Function: Determine number of nodes required, following a complete-nodes-only policy or a take-any-core-from-any-node policy
fb_determineNodes() {
    tasks="$1"         # IN (format: integer)
    nodeProperty="$2"  # IN (valid: "std", "ib", "quad", "nehalem")

    nodes=0
    if test "$FORCE_COMPLETE_NODES" -eq "1"; then
        # Only allocate complete nodes to prevent that some other cluster user
        # influences this job. Other processes possibly use much more
        # ressources than requested or even available (e.g. more memory than
        # available => swapping, cpu threading => less cpu for us, network
        # card flooding, whatever else)

        # Check ethernet and infiniband queues
        if test "${nodeProperty}" = "std" -o "${nodeProperty}" = "ib"; then
            nodes=`echo "$tasks / ($MAXCORESUSEDPERNODE * $EXTRACORESPERNODEFACTOR)" | bc`;
            # If number of CPUs per node ($MAXCORESUSEDPERNODE) is not divisor of the
            # number of processes requested, then there has been an arithmetic
            # remainder of the devision and we need one more (incompletely used)
            # node.
            if test `echo "$tasks % ($MAXCORESUSEDPERNODE * $EXTRACORESPERNODEFACTOR) != 0" | bc` -eq 1; then
                nodes=`expr ${nodes} + 1`
            fi

        # Check quadcore queues
        elif test "${nodeProperty}" = "quad"; then
            nodes=`echo "$tasks / ($MAXCORESUSEDPERQUADNODE * $EXTRACORESPERNODEFACTOR)" | bc`;
            # If number of CPUs per node ($MAXCORESUSEDPERQUADNODE) is not divisor of the
            # number of processes requested, then there has been an arithmetic
            # remainder of the devision and we need one more (incompletely used)
            # node.
            if test `echo "$tasks % ($MAXCORESUSEDPERQUADNODE * $EXTRACORESPERNODEFACTOR) != 0" | bc` -eq 1; then
                nodes=`expr ${nodes} + 1`
            fi
        fi

    else
        # Take any core we can get. => Faster scheduling, but possibly we run into
        # issues like out of memory, cpu overload etc. This setting is only useful
        # when debugging small problems.
        nodes=$tasks;
    fi

    echo ${nodes}
    return
}


#  Function: Create a jobfile for LiDO for a given test ID
fb_createJobfileLiDO() {
    benchmarktest="$1"  # IN
    file="$2"           # IN

    fb_createRuntestsScript "$benchmarktest"
    if test $? -ne 0; then
        return 1
    fi

    # We never need MPI in FeatFlow2
    MPI=NO

    fb_getNTASKS
    if test $? -ne 0; then
        return 2
    fi


    # Support for OpenMPI + Infiniband, OpenMPI + GigabitEthernet
    # and LAM/MPI + GigabitEthernet.
    # Detect which MPI flavour is currently in use.
    # Add according settings to the jobfile (!), not a user's rc file.
    # Otherwise it is not possible to add GE jobs with LAM/MPI and
    # OpenMPI+IB jobs at the same time to the queue. Nor would
    # it be possible to use the same MPI flavour, but different
    # compilers at the same time as all would share the same rc file.
#    sedExecMode=PBSQUEUED
    envSetting=""
    nodeProperty=""

    if test -n "$LOADEDMODULES"; then
        # Here enters hard-coded knowledge about maximum walltimes
        # of queues of LiDO (the server could be queries dynamically
        # for maximum walltimes, but this way it is easier.)
        case ":$LOADEDMODULES" in
            *:openmpi*infiniband*)
                echo "$0: Using OpenMPI."
                sedMPIEnv=OpenMPI
		nodeProperty="ib"
                interconnect=infiniband
                ;;
            *:openmpi*ethernet*)
                echo "$0: Using OpenMPI."
                sedMPIEnv=OpenMPI
		nodeProperty="std"
                interconnect=ethernet
                ;;
            # No way found yet to identify from the list of loaded modules whether
            # the quad nodes should be used. So, this switch is missing here.

            # Catch-all-else case
            *)
                # No MPI module loaded
                echo "$0: No MPI required."
                sedMPIEnv=not-needed-as-no-mpi-module-loaded
		nodeProperty="std"
                interconnect=ethernet
                ;;
        esac

        # Overwrite some settings in case use of quad queues is enforced.
        if test "$FORCE_USE_QUAD_QUEUES" -eq "1"; then
	    nodeProperty="quad"

        # Overwrite some settings in case use of nehalem queue is enforced.
        elif test "$FORCE_USE_NEHALEM_QUEUE" -eq "1"; then
	    nodeProperty="nehalem"
        fi

        nodes=`fb_determineNodes "${NTASKS}" "${nodeProperty}"`
        pbsOptionQueue=`fb_determineQueue "${nodes}" "${WALLCLOCKLIMIT}" "${nodeProperty}"`

        # Check whether we got a valid queue
        if test -z "${pbsOptionQueue}"; then
            if test -z "${interconnect}"; then
                interconnect=">>!!>> (none set) <<!!<<"
            fi
            cat <<EOF 1>&2
$0: Error: No queue available that satisfied the node, wallclock and
$0: interconnect requirements:
$0:   nodes          >= ${nodes}
$0:   wallclock time >= ${WALLCLOCKLIMIT}
$0:   interconnect    : ${interconnect}
EOF
            return 3
        else
            echo "$0: Trying to enqueue to <${pbsOptionQueue}>."
        fi

        # Now, it is known which is the target queue.
        # Check whether queue is enabled/disabled.
        queueState="`qstat -Q $pbsOptionQueue | tail -1 | awk '{print \$4}'`"
        if test "$queueState""x" = "nox"; then
            cat <<EOF 1>&2
$0: Error: Queue $pbsOptionQueue is currently closed, job scheduling
$0: is not possible. Script cancelled.
EOF
            exit 6
        fi

        if test "$FORCE_COMPLETE_NODES" -eq "1"; then
            # We prefer to allocate complete nodes. The thing is just that syntax
            # differs for quad nodes on the one hand and nodes with ethernet/infiniband nodes
            # on the other. Determine syntax from queue choosen.
            # Idea of detection: Try cutting of substring "quad" at the end of queue variable.
            # If this succeeds, we actually use one of the quad queues.
            if test "${pbsOptionQueue%quad}" != "$pbsOptionQueue"; then
                pbsOptionNodes="#PBS -l nodes=${nodes}:ppn=${CORESPERQUADNODE}:${nodeProperty}";
            else
                pbsOptionNodes="#PBS -l nodes=${nodes}:ppn=${CORESPERNODE}:${nodeProperty}";
            fi
            # We use the complete nodes anyway. Require all available memory
            vmem=`fb_getVMemLimitForNodeProperty "${nodeProperty}"`
            pbsOptionVMem="#PBS -l vmem=${vmem}";
        else
            # No complete nodes, just take the cores whereever we can get them.
            pbsOptionNodes="#PBS -l nodes=${nodes}:${nodeProperty}";
            # We use only partial nodes. Given that determining and storing the memory
            # requirements per benchmark test is rather tedious (also automated in the
            # meantime), but above all has proven a bit unreliable (they differ
            # significantly depending on compiler (version), MPI, BLAS/LAPACK
            # implementations), we just ask for maxAvailPerNode/coresPerNode.
            if test "${pbsOptionQueue%quad}" != "$pbsOptionQueue"; then
                divisor="${CORESPERQUADNODE}";
            else
                divisor="${CORESPERNODE}";
            fi
            vmem=`fb_getVMemLimitForNodeProperty "${nodeProperty}"`
            number=`echo ${vmem} | sed -e 's/^\(.*\)[kmgt]b$/\1/'`
            unit=`echo ${vmem} | sed -e 's/^.*\([kmgt]b$\)/\1/'`
            number=`echo ${number}/${divisor} | bc`
            pbsOptionVMem="#PBS -l vmem=${number}${unit}";
        fi


    # $LOADEDMODULES not set
    else
        cat <<EOF 1>&2
$0: Error: \$LOADEDMODULES not set. Job files cannot be created.
EOF
        exit 7
    fi

    # Evaluate command line parameter options
    #
    # Send mail on job begin, abort and end?
    pbsOptionMail=""
    if test "$DOSENDMAIL" -eq "1"; then
      pbsOptionMail="#PBS -m bae"
    fi

    # Let the job depend on another job?
    pbsOptionAfter=""
    if test "$AFTERJOB" != ""; then
      pbsOptionAfter="#PBS -W depend=afterok:$AFTERJOB"
    fi

    # Now, create the jobfile.
    LOG_BASE_DIRECTORY=`$GNUMAKE --no-print-directory print-base-log-dir`
    cat <<EOF > $file
#!/bin/sh

#PBS -l walltime=$WALLCLOCKLIMIT
$pbsOptionNodes
$pbsOptionVMem
#PBS -q $pbsOptionQueue
#PBS -o $PWD/$LOG_BASE_DIRECTORY/output.$benchmarktest
# join standard output and error streams
#PBS -j oe
#PBS -M $EMAILADDRESS
$pbsOptionMail
$pbsOptionAfter

cd $PWD

EOF
    if test $? -ne 0; then
        cat <<EOF 1>&2
$0: Error while creating jobfile.
EOF
        exit 8
    fi


    # Add environment settings to jobfile
    echo "$envSetting" >> $file
    echo >> $file
    fb_addModuleInstructions >> $file
    echo >> $file

    # Add the content of our runtests script to the jobfile, but
    # * omit the first lines (i.e. the shebang, leading comments)
    # * set execution mode to PBSQUEUED
    # * set MPI environment to use
#         s|^LOGDIR=[ ]*\.|LOGDIR=$sedLogDir|i;
#         s|^EXECMODE=PARALLEL|EXECMODE=$sedExecMode|i; \
    sed -e \
        "1,2d; \
         s|^MPIENV=[ ]*$|MPIENV=$sedMPIEnv|i;" $BENCHMARKRUNSCRIPT \
        >> $file


    # Make jobfile executable
    chmod 750 $file

    echo $0": Jobfile <$jobfile> created."
}


#  Function: Create jobfiles for LiDO for all given test IDs, bypassing the
#  usual mechanism to avoid repeated calls of 'make create-script' which slows
#  processing down by an order of magnitude (because both the initialisation
#  step of invoking make is expensive as well as repeatedly parsing all *.fbdef
#  files given the large amount of test definitions we set up by now)
fb_fastCreateAndSubmitJobfilesLiDO() {
    jobfileprefix="$1"   # IN

    # Check whether GNUmakefile is available (i.e. whether configure has been run)
    if test ! -s GNUmakefile; then
        cat <<EOF 1>&2
$0: Error: No GNUmakefile found. Please run './configure' first.
EOF
        exit 9
    fi

    # We never need MPI in FeatFlow2
    MPI=NO

    echo $0": Creating all jobfiles at once."

    # Support for
    # * OpenMPI,
    # * LAM/MPI,
    # * MPICH,
    # * MVAPICH,
    # * plain serial
    # Detect which MPI flavour is currently in use.
    # Add according settings to the jobfile (!), not a user's rc file.
    # Otherwise it is not possible to add GE jobs with LAM/MPI and
    # OpenMPI+IB jobs at the same time to the queue. Nor would
    # it be possible to use the same MPI flavour, but different
    # compilers at the same time as all would share the same rc file.
    envSetting=""
    nodeProperty=""
    if test -n "$LOADEDMODULES"; then
        case ":$LOADEDMODULES" in
            *:openmpi*infiniband*)
                echo "$0: Using OpenMPI for all job files."
                sedMPIEnv=OpenMPI
		nodeProperty="ib"
                interconnect=infiniband
                ;;
            *:openmpi*ethernet*)
                echo "$0: Using OpenMPI for all job files."
                sedMPIEnv=OpenMPI
		nodeProperty="std"
                interconnect=ethernet
                ;;
            *)
                # No MPI module loaded
                echo "$0: MPI not required for any job file."
                sedMPIEnv=not-needed-as-no-mpi-module-loaded
		nodeProperty="std"
                interconnect=ethernet
                ;;
        esac

    # $LOADEDMODULES not set
    else
        cat <<EOF 1>&2
$0: Error: \$LOADEDMODULES not set. Job files cannot be created.
EOF
        exit 10
    fi


    echo $0": Creating template for all job files..."
    # Create a unique temporary file
    # (Unfortunately 'mktemp' does not support suffixes.
    #  So, get a temporary name, append the suffix ".fbconf", check whether
    #  such a file already exists. If so, remove the temporary file created
    #  by mktemp (the one without the suffix), create a new one and repeat
    #  the test.)
    tmpfile="`mktemp feat2.benchmark.schedulefile.XXXXX`";
    trap "rm -f ${tmpfile}; exit 11" 2 3 9
    while test -e ${tmpfile}.fbconf; do
        rm -f ${tmpfile}
	tmpfile="`mktemp feat2.benchmark.schedulefile.XXXXX`";
    done
    rm -f ${tmpfile}
    tmpfile="${tmpfile}.fbconf";
    echo $ALLTESTS | tr ' ' '\n' > $tmpfile;

    # Create part of job scripts that is identical for all tests (for a given
    # choice of MPI/no MPI and build ID)
    $GNUMAKE --no-print-directory create-script-header COMPILERVERSION=${COMPILERVERSION}
    if test $? -ne 0; then
        cat <<EOF 1>&2
$0: Error while creating template for job files.
EOF
        rm -f $BENCHMARKRUNSCRIPT
        exit 12
    fi

    # Overwrite some settings in case use of quad queues is enforced.
    if test "$FORCE_USE_QUAD_QUEUES" -eq "1"; then
	nodeProperty="quad"

    # Overwrite some settings in case use of nehalem queue is enforced.
    elif test "$FORCE_USE_NEHALEM_QUEUE" -eq "1"; then
	nodeProperty="nehalem"
    fi


    # Already patch the template BENCHMARKRUNSCRIPT for execution on LiDO:
    # * omit the first lines (i.e. the shebang, leading comments)
    # * set queueing system and MPI env based on loaded modules.
    sed -i -e \
                "1,2d; \
                 s|^MPIENV=[ ]*$|MPIENV=$sedMPIEnv|i;" $BENCHMARKRUNSCRIPT
    echo $0": Creating template for all job files... Done."

    # Duplicate header for all jobfiles to be created
    echo $0": Cloning template job file..."
    trap "rm -f ${tmpfile} _tmp_.$BENCHMARKRUNSCRIPT.*; exit 13" 2 3 9
    for testid in $ALLTESTS
    do
        cp -fp $BENCHMARKRUNSCRIPT _tmp_.$BENCHMARKRUNSCRIPT.$testid
    done
    if test $? -ne 0; then
        cat <<EOF 1>&2
$0: Error while cloning template job file.
EOF
        rm -f $BENCHMARKRUNSCRIPT _tmp_.$BENCHMARKRUNSCRIPT.*
        exit 14
    fi
    echo $0": Cloning template job file... Done."

    # Now, replicate the actions of the make target 'create-script-body'
    #
    # Determine build ID (without MPI token)
    ID=`$GNUMAKE --no-print-directory .idonly`
    #
    # Determine base dir for all log files
    LOG_BASE_DIRECTORY=`$GNUMAKE --no-print-directory print-base-log-dir`
    #
    # Call the script that includes the appropriate environment variables for
    # a given test, but unlike in the GNUmakefile call it with additional command
    # line parameters instructing it to not write to screen, but to append
    # directly to the temporary files just created.
    export ID LOG_BASE_DIRECTORY MPI OVERWRITE_LOG_DIRECTORY;
    include/create_script.pl --append-to-files "_tmp_.$BENCHMARKRUNSCRIPT." $tmpfile
    rm $tmpfile
    trap "rm -f _tmp_.$BENCHMARKRUNSCRIPT.*; exit 15" 2 3 9

    # Now, turn all these temporary files into proper PBS jobscripts
    for testid in $ALLTESTS
    do
        # Ignore commented out test IDs - create_script.pl has, too.
        # So, there is no point in listing them among tests that had errors.
        if test "`echo $testid | cut -c1`" = "#"; then
            rm -f $tmpjobfile;
            continue;
        fi
        jobfile="$jobfileprefix.$testid.sh"
        tmpjobfile="_tmp_.$BENCHMARKRUNSCRIPT.$testid"
        if diff -q $BENCHMARKRUNSCRIPT $tmpjobfile 1>/dev/null; then
            # Files do not differ, so include/create_script.pl did not append anything
            # which is an indicator for the test ID having had errors.
            testIDsWithErrors="$testIDsWithErrors $testid"
            rm -f $tmpjobfile;

        else
            echo $0": Creating jobfile for <$testid>."

            # The still remaining action of the make target 'create-script':
            # Add footer function to temporary jobfile
            (echo; echo fb_footer; echo; ) >> $tmpjobfile

            fb_getNTASKS $tmpjobfile
            if test $? -ne 0; then
                echo $0": An error occurred creating the jobfile <$jobfile>."
                testIDsWithErrors="$testIDsWithErrors $testid"
                rm -f $tmpjobfile
                continue
            fi

            nodes=`fb_determineNodes "${NTASKS}" "${nodeProperty}"`
            pbsOptionQueue=`fb_determineQueue "${nodes}" "${WALLCLOCKLIMIT}" "${nodeProperty}"`

            # Check whether we got a valid queue
            if test -z "${pbsOptionQueue}"; then
                if test -z "${interconnect}"; then
                    interconnect=">>!!>> (none set) <<!!<<"
                fi
                cat <<EOF 1>&2
$0: Error: No queue available that satisfied the node, wallclock and
$0: interconnect requirements:
$0:   nodes          >= ${nodes}
$0:   wallclock time >= ${WALLCLOCKLIMIT}
$0:   interconnect    : ${interconnect}
EOF
                testIDsWithErrors="$testIDsWithErrors $testid"
                rm -f $tmpjobfile
                continue
            else
                echo "$0: Trying to enqueue to <${pbsOptionQueue}>."
            fi

            if test "$FORCE_COMPLETE_NODES" -eq "1"; then
                # We prefer to allocate complete nodes. The thing is just that syntax
                # differs for quad nodes on the one hand and nodes with ethernet/infiniband nodes
                # on the other. Determine syntax from queue choosen.
                # Idea of detection: Try cutting of substring "quad" at the end of queue variable.
                # If this succeeds, we actually use one of the quad queues.
                if test "${pbsOptionQueue%quad}" != "$pbsOptionQueue"; then
                    pbsOptionNodes="#PBS -l nodes=${nodes}:ppn=${CORESPERQUADNODE}:${nodeProperty}";
                else
                    pbsOptionNodes="#PBS -l nodes=${nodes}:ppn=${CORESPERNODE}:${nodeProperty}";
                fi
                # We use the complete nodes anyway. Require all available memory
		vmem=`fb_getVMemLimitForNodeProperty "${nodeProperty}"`
                pbsOptionVMem="#PBS -l vmem=${vmem}";
            else
                # No complete nodes, just take the cores whereever we can get them.
                pbsOptionNodes="#PBS -l nodes=${nodes}:${nodeProperty}";
                # We use only partial nodes. Given that determining and storing the memory
                # requirements per benchmark test is rather tedious (also automated in the
                # meantime), but above all has proven a bit unreliable (they differ
                # significantly depending on compiler (version), MPI, BLAS/LAPACK
                # implementations), we just ask for maxAvailPerNode/coresPerNode.
                if test "${pbsOptionQueue%quad}" != "$pbsOptionQueue"; then
                    divisor="${CORESPERQUADNODE}";
                else
                    divisor="${CORESPERNODE}";
                fi
		vmem=`fb_getVMemLimitForNodeProperty "${nodeProperty}"`
                number=`echo ${vmem} | sed -e 's/^\(.*\)[kmgt]b$/\1/'`
                unit=`echo ${vmem} | sed -e 's/^.*\([kmgt]b$\)/\1/'`
                number=`echo ${number}/${divisor} | bc`
                pbsOptionVMem="#PBS -l vmem=${number}${unit}";
            fi

            # Send mail on job begin, abort and end?
            pbsOptionMail=""
            if test "$DOSENDMAIL" -eq "1"; then
              pbsOptionMail="#PBS -m bae"
            fi

            # Let the job depend on another job?
            pbsOptionAfter=""
            if test "$AFTERJOB" != ""; then
                pbsOptionAfter="#PBS -W depend=afterok:$AFTERJOB"
            fi


            # Now, create the jobfile.
            cat <<EOF > $jobfile
#!/bin/sh

#PBS -l walltime=$WALLCLOCKLIMIT
$pbsOptionNodes
$pbsOptionVMem
#PBS -q $pbsOptionQueue
#PBS -o $PWD/$LOG_BASE_DIRECTORY/output.$testid
# join standard output and error streams
#PBS -j oe
#PBS -M $EMAILADDRESS
$pbsOptionMail
$pbsOptionAfter

cd $PWD

EOF
            if test $? -ne 0; then
                cat <<EOF 1>&2
$0: Error while creating jobfile.
EOF
                rm -f _tmp_.$BENCHMARKRUNSCRIPT.*
                exit 17
            fi


            # Add environment settings to jobfile
            echo "$envSetting" >> $jobfile
            echo >> $jobfile
            fb_addModuleInstructions >> $jobfile
            echo >> $jobfile


	    # Add the content of our runtests script to the jobfile
            cat $tmpjobfile >> $jobfile
            rm -f $tmpjobfile;

            # Make jobfile executable
            chmod 750 $jobfile

            echo $0": Jobfile <$jobfile> created."
            successfulTests=`expr $successfulTests + 1`

            if [ $NOSCHEDULE -ne "1" ]; then
                # Check whether queue is enabled/disabled.
                queueState="`qstat -Q $pbsOptionQueue | tail -1 | awk '{print \$4}'`"
                if test "$queueState""x" = "nox"; then
                    cat <<EOF 1>&2
$0: Error: Queue $pbsOptionQueue is currently closed, job scheduling
$0: is not possible. Script cancelled.
EOF
                    exit 18
                fi
                echo $0": Submitting <$jobfile>."
                # Submit the jobfile. Keep trying to submit it in
                # case the maximum number of jobs per user is already
                # reached.
                fb_submitPBSJobfile "$jobfile" "Maximum number of jobs already in queue"
            else
                echo $0": Submitting <$jobfile> skipped."
            fi
        fi
    done
    trap - 2 3 9;
}




# ================================================
# = Here is where to script really gets executed =
# ================================================

fb_readargs "$@"
fb_ensureCorrectWorkingDir
fb_setMailSender
fb_findGNUMake
fb_queryPBSLiDO
echo

successfulTests=0
testIDsWithErrors=""

# Intelligently create jobfile. Strategy: Bypass the usual mechanism
# involving repeated calls of 'make singletest' to speed up processing.
#
# Takes 3.5min for 1010 tests.
fb_fastCreateAndSubmitJobfilesLiDO "$JOBFILEPREFIX"
# Show error messages, if any
fb_postprocess "$successfulTests" "$testIDsWithErrors"

# # Create jobfiles one by one. Basically following the strategy:
# #       for testid in $ALLTESTS
# #           echo $testid > singletest.fbconf
# #           make singletest
# #           rm -f singletest.fbconf
# #       done
# # Takes > 60min for 1010 tests.
# for testid in $ALLTESTS
# do
#     jobfile="$JOBFILEPREFIX.$testid.sh"
#     echo $0": Creating jobfile for <$testid>."
#     fb_createJobfileLiDO "$testid" "$jobfile"
#     if test $? -eq 0; then
#       successfulTests=`expr $successfulTests + 1`
#       if [ $NOSCHEDULE -ne "1" ]; then
#           echo $0": Submitting <$jobfile>."
#           # Submit the jobfile. Keep trying to submit it in
#           # case the maximum number of jobs per user is already
#           # reached.
#           fb_submitPBSJobfile "$jobfile" "Maximum number of jobs already in queue"
#       else
#           echo $0": Submitting <$jobfile> skipped."
#       fi
#     else
#       echo $0": An error occurred creating the jobfile. Submit cancelled."
#       testIDsWithErrors="$testIDsWithErrors $testid"
#     fi
#     echo
# done
# # Show error messages, if any
# fb_postprocess "$successfulTests" "$testIDsWithErrors"
